# ğŸ”¥K8Så»ºç½®æ¶æ§‹èªªæ˜

## ä»€éº¼æ˜¯RKE2

>  **(Rancher Kubernetes Engine)**
>> *  RKE2æ˜¯ä¸€ç¨®Kubernetes distribution. 
>> *  [CNCF Certified partners and providers.](https://landscape.cncf.io/?group=certified-partners-and-providers&view-mode=grid)
>> *  RKE2ä»¥Apache-2.0 license é–‹æº.
>> *  [RKE2æ˜¯SUSEæ——ä¸‹çš„é–‹æºå°ˆæ¡ˆä¹‹ä¸€.](https://www.rancher.com/projects)
>> *  [RKE2åŒæ™‚æä¾›å•†æ¥­æ”¯æ´.](https://www.rancher.com/quick-start)
>> *  [åœ‹å…§ä»£ç†å» å•†](https://www.palsys.com.tw/product_detail.php?id=186)

## RKE2 æ¶æ§‹

[Kubenetes å®˜æ–¹åƒè€ƒ](https://kubernetes.io/zh-cn/docs/concepts/architecture/)

![RKE2 Arch](./img/rek2-arch.png)

## RKE2 HA æ¶æ§‹

![RKE2 HA Arch](./img/rke2-HA-arch.png)

- - -
# ğŸ”¥RKE2 with kube-vip å®‰è£
- - -

## RKE2 å®‰è£

[RKE2å®‰è£éœ€æ±‚](https://docs.rke2.io/install/requirements)

__å®‰è£è¦åŠƒ__

|Node Name|  Description  |     IP    |
|---------|---------------|-----------|
|kube vip | cluster VIP   | 10.0.1.100|
|master1  | control plane | 10.0.1.101|
|master2  | control plane | 10.0.1.102|
|master3  | control plane | 10.0.1.103|
|gpu-node | gpu provider  | 10.0.1.104|
|harbor   | registry      | 10.0.1.105|
|TrueNas  | storage       | 10.0.1.106|

__å¯åƒè€ƒNutanixå®˜ç¶²çš„å®‰è£æ–¹æ³•__

[Deploy Highly Available RKE2 with Kube-VIP](https://portal.nutanix.com/page/documents/solutions/details?targetId=BP-2103-Rancher-SUSE-Nutanix:deploy-highly-available-rke2-with-kube-vip.html)

__ä»¥root èº«ä»½é€²è¡Œå®‰è£__

```shell
sudo su -
```

__å®‰è£ç’°å¢ƒè®Šæ•¸__

| Environment Variable    | Description |
| ------------------------| ----------- |
| INSTALL_RKE2_VERSION    | Version of RKE2 to download from GitHub. Will attempt to download the latest release from the stable channel if not specified. INSTALL_RKE2_CHANNEL should also be set if installing on an RPM-based system and the desired version does not exist in the stable channel.   |
| INSTALL_RKE2_TYPE       | Type of systemd service to create, can be either "server" or "agent" Default is "server".        |
| INSTALL_RKE2_CHANNEL_URL| Channel URL for fetching RKE2 download URL. Defaults to https://update.rke2.io/v1-release/channels.|
| INSTALL_RKE2_CHANNEL    | Channel to use for fetching RKE2 download URL. Defaults to stable. Options include: stable, latest, testing. |
| INSTALL_RKE2_METHOD     | Method of installation to use. Default is on RPM-based systems rpm, all else tar. |

__å»ºç«‹ç’°å¢ƒè®Šæ•¸__

```shell
export RKE2_API_VIP=<API_SERVER_VIP_IP>
export RKE2_NODE_0_IP=<CONTROL_PLANE_FIRST_NODE_IP>
export RKE2_NODE_1_IP=<CONTROL_PLANE_SECOND_NODE_IP>
export RKE2_NODE_2_IP=<CONTROL_PLANE_THIRD_NODE_IP>
export NODE_JOIN_TOKEN=`echo "$(uuidgen)::$(openssl rand -hex 16)"`
export INTERFACE=ens18
export KUBE_VIP_VERSION=v0.8.9
```

__å»ºç«‹RKE2 æ‰€éœ€ç›®éŒ„__

```shell
mkdir -p /etc/rancher/rke2
mkdir -p /var/lib/rancher/rke2/server/manifests/
```

__å»ºç«‹Rancher RKE2 config.yaml (å¯ä»¥æ ¹æ“šæ‰€éœ€ç’°å¢ƒè¨­å®šdisableé¸é …)__

```shell
cat <<EOF | tee /etc/rancher/rke2/config.yaml
token: ${NODE_JOIN_TOKEN}
tls-san:
- ${HOSTNAME}
- ${RKE2_API_VIP}
- ${RKE2_NODE_0_IP}
- ${RKE2_NODE_1_IP}
- ${RKE2_NODE_2_IP}
write-kubeconfig-mode: 600
EOF
```

__æŒ‡å®šå®‰è£ç‰ˆæœ¬:__
[RKE2 release](https://github.com/rancher/rke2/releases)
```shell
   ## scriptæœƒæª¢æŸ¥ç’°å¢ƒ, å¦‚ä¸èƒ½ç”¨RPMå®‰è£,æœƒä¸‹è¼‰tar ballã€‚
   curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.31.4+rke2r1 sh -
```

__ä¸å‚³ä¹‹ç§˜__

[CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYSæ˜¯ä¸€å€‹è¢«éš è—çš„ç’°å¢ƒè®Šæ•¸](https://github.com/rancher/rke2/discussions/6639)
```shell
ç¶­è­·è€…çš„è©±:
 Use of CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYS is NOT officially supported, and we DO NOT document it or recommend it. However, we have no plans to remove support for this env var from the library that RKE2 uses to generate certificates.

ç¶­è­·è€…çš„çœŸå¿ƒè©±:
 Yes. You should be patching your nodes at least every few months. After a full year your Kubernetes minor version would be end of life and long overdue for an upgrade.

 Kubernetes is not a good choice for those who want to install things and then never touch it again.

å¦‚æœä½ çœŸçš„æƒ³ç”¨çš„è©±...
$ echo CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYS=3650 >> /usr/local/lib/systemd/system/rke2-server.env
systemctl enable rke2-server.service
systemctl start rke2-server.service
```

__å•Ÿç”¨RKE2 æœå‹™__

```shell
systemctl enable rke2-server.service
systemctl start rke2-server.service
```

__æŸ¥çœ‹logæ˜¯å¦å·²å•Ÿç”¨æˆåŠŸ__

```shell
journalctl -u rke2-server -f
```

__ç¢ºèªRKE2 æœå‹™å·²å•Ÿç”¨__

```shell
systemctl status rke2-server
```

__æª¢æŸ¥NODEæ˜¯å¦å·²ç¶“Ready__

```shell
export PATH=$PATH:/var/lib/rancher/rke2/bin
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock

kubectl get nodes -o wide
```

## kube-vipå®‰è£

__å»ºç«‹kube-vipçš„RBACæ¸…å–®__

```shell
curl https://kube-vip.io/manifests/rbac.yaml > /var/lib/rancher/rke2/server/manifests/kube-vip-rbac.yaml
```

__æŠ“å–kube-vip imageå’Œè¨­å®šåˆ¥å__

```shell
crictl pull docker.io/plndr/kube-vip:$KUBE_VIP_VERSION

alias kube-vip="ctr --namespace k8s.io run --rm --net-host docker.io/plndr/kube-vip:$KUBE_VIP_VERSION vip /kube-vip"

```

__å»ºç«‹Kube-VIP Daemonset static pod manifests__

```shell
kube-vip manifest daemonset \
    --interface $INTERFACE \
    --address $RKE2_API_VIP \
    --inCluster \
    --taint \
    --controlplane \
    --services \
    --arp \
    --leaderElection | tee /var/lib/rancher/rke2/server/manifests/kube-vip.yaml
```

__æª¢æŸ¥kube-vipæ˜¯å¦å·²éƒ¨ç½²æˆåŠŸ__

```shell
kubectl get ds -n kube-system kube-vip-ds
```

__åœ¨å…¶é¤˜äºŒå°NODEé‡è¦†ä¸Šè¿°å‹•ä½œ, é™¤äº†config.yamlå†å¤šæ–°å¢serveré¸é …__

```shell
cat <<EOF | tee /etc/rancher/rke2/config.yaml
server: https://${RKE2_API_VIP}:9345
token: ${NODE_JOIN_TOKEN}
tls-san:
- ${HOSTNAME}
- ${RKE2_API_VIP}
- ${RKE2_NODE_0_IP}
- ${RKE2_NODE_1_IP}
- ${RKE2_NODE_2_IP}
write-kubeconfig-mode: 600
EOF
```

## RKE2 work nodeå®‰è£

__å®‰è£agent__

```shell
   curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="agent" INSTALL_RKE2_VERSION=v1.31.4+rke2r1 sh -
```

__å»ºç«‹RKE2 æ‰€éœ€ç›®éŒ„__

```shell
mkdir -p /etc/rancher/rke2
```

__ç·¨è¼¯/etc/rancher/rke2/config.yaml__

```shell
server: https://${RKE2_API_VIP}:9345
token: ${NODE_JOIN_TOKEN}
```

__ç›´æ¥å•Ÿç”¨rke2-agentæœå‹™__

```shell
   systemctl enable rke2-agent.service
   systemctl start rke2-agent.service
```

__ä½¿ç”¨kubectlå·¥å…·ç¨‹å¼, æŸ¥çœ‹ç›®å‰çš„å…ƒä»¶ç‹€æ…‹__

```shell
   kubectl get --raw='/readyz?verbose'
```

__æª¢æŸ¥æ˜¯å¦å·²åŠ å…¥cluster__

```shell
   kubectl get nodes
```

- - -
# ğŸ”¥Deploy NVIDIA GPU operator
- - -


__ä»€éº¼æ˜¯ GPU operator__

[NVIDIA GPU Operator Overview](https://www.youtube.com/watch?v=KER0dbfmAqQ)

![nvidia gpu operator intro](./img/nvidia_gpu_operator_intro.png)

__å®‰è£ä¹‹å‰éœ€å…ˆå®‰è£nvidia driver__

[NVIDIA å®˜æ–¹ä¸‹è¼‰](https://www.nvidia.com/zh-tw/drivers/)

```shell
/* æŸ¥çœ‹driveråŠCUDAç‰ˆæœ¬ */

# nvidia-smi
Mon Mar 31 15:33:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce 940MX           Off |   00000000:02:00.0 Off |                  N/A |
| N/A   40C    P8             N/A /  200W |      29MiB /   2048MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      4042      G   /usr/lib/xorg/Xorg                              2MiB |
|    0   N/A  N/A      4320    C+G   ...libexec/gnome-remote-desktop-daemon         20MiB |
+-----------------------------------------------------------------------------------------+
```

## ä½¿ç”¨helmä¾†éƒ¨ç½²nvidia gpu operator

[helm å®˜ç¶²](https://helm.sh/)

```
helmæ˜¯Kubernetesçš„å¥—ä»¶ç®¡ç†å™¨ï¼Œchartå‰‡ç‚ºè©²å¥—ä»¶æ ¼å¼ã€‚
```

__Helm Version Support Policy__

[helm veison support policy](https://helm.sh/zh/docs/topics/version_skew/)

```
Helmçš„ç‰ˆæœ¬ä»¥x.y.zæ–¹å¼æè¿°ï¼Œxç‚ºä¸»ç‰ˆæœ¬ï¼Œyç‚ºæ¬¡è¦ç‰ˆæœ¬ï¼Œzå‰‡ç‚ºpatchç‰ˆæœ¬ï¼Œéµå¾ªSemantic Versioningã€‚
å¾helm3é–‹å§‹ï¼Œ æœƒèˆ‡kubernetesä¿æŒn-3ç‰ˆæœ¬çš„ç›¸å®¹ï¼Œ ä¾‹å¦‚ï¼š helmç‰ˆæœ¬ç‚º3.12.x, å‰‡å°æ‡‰kubernetesçš„
ç›¸å®¹ç‰ˆæœ¬ç‚º1.27.x - 1.24.xã€‚
```

## helmå®‰è£æ–¹å¼ 
 
* Binary æ–¹å¼å®‰è£

  1. ç›´æ¥å¾githubä¸‹è¼‰: https://github.com/helm/helm/releases
  2. ç›´æ¥è§£å£“ç¸® ï¼ˆtar -zxvf helm-v3.0.0-linux-amd64.tar.gzï¼‰
  3. ç§»å‹•åˆ°æ‰€è¦ç›®éŒ„ä½ç½®ï¼ˆmv linux-amd64/helm /usr/local/bin/helmï¼‰

* Script æ–¹å¼å®‰è£

  ```shell
  $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
  $ chmod 700 get_helm.sh
  $ ./get_helm.sh
  ```

__æ–°å¢ä¸€å€‹repo__

```shell
### å°‡ä¸€å€‹chartå€‰åº«urlåŠ å…¥
$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories
```

__æŸ¥çœ‹è©²å€‰åº«çš„chart åˆ—è¡¨__
```shell
$ helm search repo bitnami

NAME                                        	CHART VERSION	APP VERSION  	DESCRIPTION                                       
bitnami/airflow                             	22.7.0       	2.10.5       	Apache Airflow is a tool to express and execute...
bitnami/apache                              	11.3.4       	2.4.63       	Apache HTTP Server is an open-source HTTP serve...
bitnami/apisix                              	4.2.0        	3.11.0       	Apache APISIX is high-performance, real-time AP...
bitnami/appsmith                            	5.2.2        	1.64.0       	Appsmith is an open source platform for buildin...
bitnami/argo-cd                             	7.3.0        	2.14.7       	Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                      	11.1.10      	3.6.5        	Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                         	6.3.5        	8.0.14       	ASP.NET Core is an open-source framework for we...
bitnami/cassandra                           	12.2.2       	5.0.3        	Apache Cassandra is an open source distributed ...
```

__æ–°å¢ NVIDIA Helm å„²å­˜åº«__

```shell
$ helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && helm repo update
```

__å®‰è£gpu operator__

```shell
 $ helm install --wait --generate-name \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator \
    --version=v25.3.2 \
    --set operator.defaultRuntime=containerd \
    --set driver.enabled=false \
    --set toolkit.env[0].name=CONTAINERD_CONFIG \
    --set toolkit.env[0].value=/var/lib/rancher/rke2/agent/etc/containerd/config.toml \
    --set toolkit.env[1].name=CONTAINERD_SOCKET \
    --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \
    --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \
    --set toolkit.env[2].value=nvidia \
    --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \
    --set-string toolkit.env[3].value=true
    
```

__æª¢æŸ¥è©²GPU nodeæ˜¯å¦å·²åµæ¸¬åˆ°GPUåŠdriver__

```shell
 $ kubectl get node $NODENAME -o jsonpath='{.metadata.labels}' | jq | grep "nvidia.com"
   "nvidia.com/cuda.driver-version.full": "550.120",
   "nvidia.com/cuda.driver-version.major": "550",
   "nvidia.com/cuda.driver-version.minor": "120",
   "nvidia.com/cuda.driver-version.revision": "",
   "nvidia.com/cuda.driver.major": "550",
   "nvidia.com/cuda.driver.minor": "120",
   "nvidia.com/cuda.driver.rev": "",
   "nvidia.com/cuda.runtime-version.full": "12.4",
   "nvidia.com/cuda.runtime-version.major": "12",
   "nvidia.com/cuda.runtime-version.minor": "4",
   "nvidia.com/cuda.runtime.major": "12",
   "nvidia.com/cuda.runtime.minor": "4",
   "nvidia.com/gfd.timestamp": "1743408700",
   "nvidia.com/gpu.compute.major": "5",
   "nvidia.com/gpu.compute.minor": "0",
   ............................................
   ............................................
   ............................................(çœç•¥)
```

__æª¢æŸ¥container runtime binaryå·²è¢«operatorå®‰è£__

```shell
/* åœ¨GPU nodeä¸Šæª¢æŸ¥ */
$ ls /usr/local/nvidia/toolkit/nvidia-container-runtime
```

__æª¢æŸ¥containerd config å·²è¢«æ›´æ–°__

```shell
# grep nvidia /var/lib/rancher/rke2/agent/etc/containerd/config.toml
plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/local/nvidia/toolkit/nvidia-container-runtime"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia-cdi"]
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia-cdi".options]
  BinaryName = "/usr/local/nvidia/toolkit/nvidia-container-runtime.cdi"
```

__å»ºç«‹ä¸€å€‹Podä¾†æ¸¬è©¦GPU__

```shell
kubectl apply -f -<<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF
```

__æª¢æŸ¥ pod é‹è¡Œçš„æ—¥èªŒ:__

```shell
$ kubectl logs pod/nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=<numBodies>]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance) 
	-numbodies=<N>    (number of bodies (>= 1) to run in simulation) 
	-device=<d>       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=<i>   (where i=(number of CUDA devices > 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=<file.bin> (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

> Windowed mode
> Simulation data stored in video memory
> Single precision floating point simulation
> 1 Devices used for simulation
GPU Device 0: "Maxwell" with compute capability 5.0

> Compute 5.0 CUDA device: [NVIDIA GeForce 940MX]
3072 bodies, total time for 10 iterations: 3.641 ms
= 25.917 billion interactions per second
= 518.344 single-precision GFLOP/s at 20 flops per interaction
```

__ä¸ç”¨runtimeclassname__

```
kubectl run gpu-test --rm -t -i --restart=Never --image=nvcr.io/nvidia/cuda:10.1-base-ubuntu18.04 nvidia-smi
```

__è¨­å®šdefault runtimeclass__

```shell
/* ç·¨è¼¯/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl */

[plugins."io.containerd.grpc.v1.cri".containerd]
  default_runtime_name = "nvidia"

/* é‡æ–°å•Ÿå‹•æœå‹™ */
 $ systemctl restart rke2-agent.service

/* ç¢ºèªdefault runtimeæ˜¯å¦å·²ç‚ºnvidia */
$ crictl info | jq '.config.containerd.defaultRuntimeName'
```

- - -
# ğŸ”¥ Harbor å®‰è£
- - -

[Harbor å®˜ç¶²](https://goharbor.io/)

> Harboræ˜¯ä¸€ç¨®registry(ç”¨ä»¥å­˜æ”¾container imageçš„åœ°æ–¹), VMware æç»çµ¦CNCF,ä½¿ç”¨
> open source (Apach-2.0 license)é‡‹å‡º, ä»¥RBCAæ§åˆ¶å­˜å–, ä¸¦æƒç„imageç¢ºä¿æ²’æœ‰æ¼æ´,
> è€Œä¸”å°‡imageæ¨™è¨˜ç‚ºå¯ä¿¡ä»»çš„ã€‚ 

__Harwareéœ€æ±‚__

|Resource|Minimum|Recommended|
|--------|-------|-----------|
|CPU     |2 CPU  |4 CPU      |
|Mem     |4 GB   |8 GB       |
|Disk    |40 GB  |160 GB     |

__Softwareéœ€æ±‚__

|Software      |Version              |Description                                                          |
|--------------|---------------------|---------------------------------------------------------------------|                                       
|Docker Engine |Version > 20.10      |[Docker Engine Installation](https://docs.docker.com/engine/install/)|
|Docker Compose|Docker compose > 2.3 |Docker Compose is part of Docker Engine                              |
|OpenSSL       |Latest (optional)    |Used to generate certificate and keys for Harbor                     |

__Network Port__

|Port|Protocol|Description                                                                                                       |
|----|--------|------------------------------------------------------------------------------------------------------------------|
|443 |HTTPS   |Harbor portal and core API accept HTTPS requests on this port. You can change this port in the configuration file.|
|80  |HTTP    |Harbor portal and core API accept HTTP requests on this port. You can change this port in the configuration file. |

## å®‰è£ 

*  ç›´æ¥è‡³[github](https://github.com/goharbor/harbor/releases?page=1)ä¸‹è¼‰æ‰€è¦çš„ç‰ˆæœ¬, æœ‰åˆ†onlineæˆ–offlineäºŒç¨®æ–¹å¼ã€‚
*  å®‰è£harborä¹‹å‰éœ€å…ˆå®‰è£å¥½dockerã€‚
```shell
 /* è§£å£“ç¸®æª”æ¡ˆ */
 # tar -zxvf harbor-online-installer-v2.12.2.tgz 
 # cd harbor

 /* è¤‡è£½harbor.yml.tmpl */
 # cp harbor.yml.tmpl harbor.yml

 /* ç·¨è¼¯harbor.yml 
    hostnameï¼š ç”¨ä»¥å­˜å–admin UIå’Œregistry serviceï¼ŒIP æˆ– FQDN.
    certificate: æ†‘è­‰ä½ç½®ã€‚
    private_key: ç§é‘°ä½ç½®ã€‚           
*/

 hostname: 10.0.1.105
 certificate: /data/cert/edwin.io.crt
 private_key: /data/cert/edwin.io.key 
 harbor_admin_password: Harbor12345     # é è¨­adminå¯†ç¢¼
 password: root123           # é è¨­DBå¯†ç¢¼   

/* å»ºç«‹/data/certç›®éŒ„ */
# mkdir -p /data/cert

/* å»ºç«‹CAæ†‘è­‰ */

1. CA certificate private key.
# openssl genrsa -out ca.key 4096

2. Generate the CA certificate.
# openssl req -x509 -new -nodes -sha512 -days 3650  -subj "/C=TW/ST=Kaohsiung/L=Kaohsiung/O=Gundam/OU=Personal/CN=Harbor Root CA"  -key ca.key  -out ca.crt

/* ç”¢ç”Ÿserveræ†‘è­‰ */
1. Generate a private key.
# openssl genrsa -out edwin.io.key 4096

2. Generate a certificate signing request (CSR).
# openssl req -sha512 -new -subj "/C=TW/ST=Kaohsiunt/L=Kaohsiung/O=Neweb/OU=Personal/CN=edwin.io" -key edwin.io.key -out edwin.io.csr

3. Generate an x509 v3 extension file.
# cat > v3.ext <<-EOF
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
DNS.1=yourdomain.com
# or IP
IP.1=10.0.1.105
DNS.2=yourdomain
DNS.3=hostname
EOF

4. Use the v3.ext file to generate a certificate for your Harbor host.
# openssl x509 -req -sha512 -days 3650 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in edwin.io.csr -out edwin.io.crt

/* å°‡ä¸Šé¢ç”¢ç”Ÿçš„æ†‘è­‰æä¾›çµ¦harboråŠdocker */

1. è¤‡è£½åˆ°harborä¸»æ©Ÿä¸Šçš„/data/certç›®éŒ„
# cp edwin.io.crt /data/cert/
# cp edwin.io.key /data/cert/

2. å°‡edwin.io.crtè½‰æ›æˆedwin.io.certçµ¦dockerä½¿ç”¨
# openssl x509 -inform PEM -in edwin.io.crt -out edwin.io.cert

3. å°‡server certificate, key and CA filesè¤‡è£½åˆ°dockerç›®éŒ„
# cp edwin.io.cert /etc/docker/certs.d/10.0.1.105/
# cp edwin.io.key  /etc/docker/certs.d/10.0.1.105/
# cp ca.crt /etc/docker/certs.d/10.0.1.105/

4. Restart Docker Engine.
# systemctl restart docker

/* è¨­å®šharbor*/
1. Run the prepare script to enable HTTPS.
# ./prepare
# ./install.sh --with-trivy (å¦‚æœå°šæœªå®‰è£)

2. If Harbor is running, stop and remove the existing instance.
# docker compose down -v

3. Restart Harbor:
# docker compose up -d

è‡³æ­¤, æ‡‰è©²å¯ä»¥æ‰“é–‹browserçœ‹åˆ°harborä»‹é¢äº†ã€‚
```

__é—œæ©Ÿ__

```shell
 /* åœ¨harborç›®éŒ„ä¸­ä¸‹æ­¤æŒ‡ä»¤ */

# docker compose stop
Stopping nginx              ... done
Stopping harbor-portal      ... done
Stopping harbor-jobservice  ... done
Stopping harbor-core        ... done
Stopping registry           ... done
Stopping redis              ... done
Stopping registryctl        ... done
Stopping harbor-db          ... done
Stopping harbor-log         ... done
```

__é–‹æ©Ÿ__

```shell
 /* åœ¨harborç›®éŒ„ä¸­ä¸‹æ­¤æŒ‡ä»¤ */

# docker compose start
Starting log         ... done
Starting registry    ... done
Starting registryctl ... done
Starting postgresql  ... done
Starting core        ... done
Starting portal      ... done
Starting redis       ... done
Starting jobservice  ... done
Starting proxy       ... done
```

__é‡æ–°è¨­å®š__

```shell
1. Stop Harbor.
# docker compose down -v

2. Update harbor.yml
# vim harbor.yml

3. Run the prepare script to populate the configuration.
# ./prepare
  æˆ–æ˜¯éœ€è¦å®‰è£Trivy,
# ./prepare --with-trivy

4. Re-create and start the Harbor instance.
# docker compose up -d

```

## æ¸¬è©¦imageä¸Šå‚³

```shell
/* loging åˆ° harbor */
# docker login https://10.0.1.105

/* å°‡æœ¬åœ°image å…ˆåštag */
# docker tag hello-world:latest 10.0.1.105/library/myapp:v1.0

/* æ¨é€imgageåˆ°harbor */
# docker push 10.0.1.105/library/myapp:v1.0
```

## æ’°å¯«systemd æœå‹™ (/etc/systemd/system/)

```shell
[Unit]
Description=Harbor Container Registry
After=docker.service systemd-networkd.service systemd-resolved.service
Requires=docker.service

[Service]
Type=simple
Restart=on-failure
RestartSec=5
WorkingDirectory=/root/harbor
ExecStart=/usr/bin/docker compose -f /root/harbor/docker-compose.yml up
ExecStop=/usr/bin/docker compose -f /root/harbor/docker-compose.yml down

[Install]
WantedBy=multi-user.target
```

## ç§æœ‰registryè¨­å®š

__åœ¨æ‰€éœ€æ‹‰å–imageçš„ç¯€é»ä¸Šç·¨è¼¯è¨­å®šæª”__

ç·¨è¼¯/etc/rancher/rke2/registries.yaml

```shell 
mirrors:
  edwin.io:
    endpoint:
      - "https://10.0.1.105"
configs:
  "10.0.1.105":
    tls:
      cert_file:        # path to the cert file used to authenticate to the registry
      key_file:         # path to the key file for the certificate used to authenticate to the registry
      ca_file:          # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: false  # may be set to true to skip verifying the registry's certificate
```

__æ”¾ç½®æ‰€éœ€æ†‘è­‰__

```shell
$  mkdir /etc/rancher/rke2/cert

/*
 è¦å…ˆå°‡harborä¸Šçš„æ†‘è­‰è¤‡è£½éä¾†,
 å°‡/etc/docker/certs.d/10.0.1.105/ç›®éŒ„ä¸‹
 çš„æ†‘è­‰å…ˆcopyåˆ°/tmp/certç›®éŒ„ä¸‹, ä¸¦å°‡edwin.io.key
 æ¬Šé™æ”¹ç‚º655, ä¹‹å¾Œå†æ”¹600å›ä¾†ã€‚
 */

$ scp edwin@10.0.1.105:/tmp/cert/* /etc/rancher/rke2/cert/
$ chmod 600 /etc/rancher/rke2/cert/edwin.io.key

/* 
é‡å•Ÿrke2æœå‹™ä½¿å…¶ç”Ÿæ•ˆ, 
å¯å°‡config.toml.tmplæ”¹ç‚ºconfig.toml.bak,
è®“æ–°çš„config.tomlç”¢ç”Ÿã€‚
*/

$ systemctl restart rke2-agent.service

```

__æ¸¬è©¦æ˜¯å¦å¯ä»¥æ‹‰å–image__

```sehll
$ kubectl run hello --rm -t -i --restart=Never --image=10.0.1.105/library/myapp:v1.0
```

- - -
# ğŸ”¥TrueNAS å®‰è£åŠè¨­å®š
- - -

[TrueNAS Community Editionå®˜ç¶²ä¸‹è¼‰](https://www.truenas.com/download-truenas-community-edition/)

__Minimum Hardware Requirements__

|Processor                                  |Memory                      |Boot Device          |Storage                                                |
|-------------------------------------------|----------------------------|---------------------|-------------------------------------------------------|
|2-Core Intel 64-Bit or AMD x86_64 processor|8 GB RAM (16 GB Recommended)|16 GB SSD boot device|Two identically-sized devices for a single storage pool|

## å®‰è£

ç‰ˆæœ¬: 24.10.2

__è¼‰å…¥å®‰è£å…‰ç¢Ÿé–‹å§‹å®‰è£__

![truenas_install-0](./img/truenas_install-0.png)

__ç›´æ¥é»é¸[OK]é–‹å§‹å®‰è£__

![truenas_install-1](./img/truenas_install-1.png)

__é¸å–æ‰€å®‰è£çš„ç¡¬ç¢Ÿ, ä¸¦é»é¸[OK]__

![truenas_install-2](./img/truenas_install-2.png)

__é»é¸[Yes]__

![truenas_install-3](./img/truenas_install-3.png)

__é»é¸[OK], é è¨­å¸³è™Ÿç‚ºtruenas_admin__

![truenas_install-4](./img/truenas_install-4.png)

__è¨­å®šå¯†ç¢¼å¾Œé»é¸[OK]__

![truenas_install-5](./img/truenas_install-5.png)

__ç¢ºèªç„¡èª¤å¾Œé»é¸[Yes]__

![truenas_install-6](./img/truenas_install-6.png)

__å®‰è£æˆåŠŸå¾Œ, é»é¸[OK]__

![truenas_install-7](./img/truenas_install-7.png)

__é»é¸[Reboot System]å¾Œå†é»é¸[OK]__

![truenas_install-8](./img/truenas_install-8.png)

__é–‹æ©Ÿå¾Œå·²æœ‰DHCP, ç™»å…¥ç¶²é é–‹å§‹è¨­å®š__

![truenas_install-9](./img/truenas_install-9.png)

## è¨­å®š

__å»ºç«‹ä¸€å€‹Pool, [Storage]->[Create pool]__

![create_pool-0](./img/create_pool-0.png)

__è¼¸å…¥poolåç¨±, æŒ‰ä¸‹[Save]__

![create_pool-1](./img/create_pool-1.png)

__é¸æ“‡RAID level, æŒ‰ä¸‹[Save]__

![create_pool-2](./img/create_pool-2.png)

__Reviewæ²’å•é¡Œå¾Œ, æŒ‰ä¸‹[Create Pool]__

![create_pool-3](./img/create_pool-3.png)

__å‹¾é¸[Confirm]->[Continue]__

![create_pool-4](./img/create_pool-4.png)

__å»ºç«‹ä¸€å€‹dataset, [Add Dataset]__

![add_dataset-0](./img/add_dataset-0.png)

__è¼¸å…¥datasetåç¨±, [Save]__

![add_dataset-1](./img/add_dataset-1.png)

__é»é¸[System]->[Service]__

![service-0](./img/service-0.png)

__é–‹å•ŸNFSåŠSSHæœå‹™__

![service-1](./img/service-1.png)

__å‹¾é¸[NFSv3 ownership model for NFSv4], æŒ‰ä¸‹[Save]__

![service-2](./img/service-2.png)

__æ–°å¢NFS Share__

__é»é¸[Share]->[Add]__

![nfs-0](./img/nfs-0.png)

__é»é¸Share path,æŒ‰ä¸‹[Save]__

![nfs-1](./img/nfs-1.png)


__ç”¢ç”Ÿsshé‡‘é‘°çµ¦rootä½¿ç”¨__

```bash
$ ssh-keygen -t rsa -C root@truenas.lab.test -f truenas_rsa

Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in truenas_rsa
Your public key has been saved in truenas_rsa.pub
The key fingerprint is:
SHA256:bCGhVnKkh+Pq/TxQNoeL75ylGzS3Aagf6cjcmvVvOtQ root@truenas.lab.test
The key's randomart image is:
+---[RSA 3072]----+
|    ..=          |
|     O .         |
|    B +..        |
|   + +=+..       |
|  . ++o=S        |
| o *ooo+Eo       |
|  = =+. o        |
| . = ++=.        |
|  + .o@B.        |
+----[SHA256]-----+

/* æœƒç”¢ç”Ÿä¸€å‰¯å…¬ç§é‘° */

truenas_rsa
truenas_rsa.pub
```

__é»é¸[Credentials]->[Users]__

![user-0](./img/user-0.png)

__å±•é–‹user__

![user-1](./img/user-1.png)

__é»é¸[Edit]__

![user-2](./img/user-2.png)

__å°‡ç”¢ç”Ÿçš„[truenas_rsa.pub]çš„keyè²¼ä¸Š, ä¸¦å°‡Shellé¸å–bash__

![user-3](./img/user-3.png)

__é»é¸å³ä¸Šæ–¹é¸å–®, ä¸¦é»é¸[API Keys]__

![api-0](./img/api-0.png)

__é»é¸[Add]__

![api-1](./img/api-1.png)

__è¼¸å…¥ä¸€å€‹åç¨±, æŒ‰ä¸‹[Save]__

![api-2](./img/api-2.png)

__æŒ‰ä¸‹[Copy to Clipboard], å°‡API keyè¤‡è£½èµ·ä¾†å‚™ç”¨__

![api-3](./img/api-3.png)

- - -
# ğŸ”¥democratic-csiå®‰è£åŠè¨­å®š
- - -

## åŠ é€²democratic-csi source

```shell
$ helm repo add democratic-csi https://democratic-csi.github.io/charts/
$ helm repo update
$ helm search repo democratic-csi/
```

## è¨­å®šå€¼

__ç·¨è¼¯ä¸€å€‹truenas-csi.yaml__

```shell
csiDriver:
  name: "org.democratic-csi.nfs"

storageClasses:
  - name: truenas-nfs-csi
    defaultClass: true
    reclaimPolicy: Retain
    volumeBindingMode: Immediate
    allowVolumeExpansion: true
    parameters:
      fsType: nfs

    mountOptions:
      - noatime
      - nfsvers=4.2
    secrets:
      provisioner-secret:
      controller-publish-secret:
      node-stage-secret:
      node-publish-secret:
      controller-expand-secret:

# if your cluster supports snapshots you may enable below
volumeSnapshotClasses: []

driver:
  config:
    # please see the most up-to-date example of the corresponding config here:
    # https://github.com/democratic-csi/democratic-csi/tree/master/examples
    # YOU MUST COPY THE DATA HERE INLINE!
    driver: freenas-nfs
    instance_id:
    httpConnection:
      protocol: http
      host: <Put your TrueNAS FQDN or fixed IP here>
      port: 80
      apiKey: <Use the api key retrieved from the TrueNAS server>
      username: root
      allowInsecure: true
      apiVersion: 2
    sshConnection:
      host: <Put your TrueNAS FQDN or fixed IP here>
      port: 22
      username: root
      # use either password or key
      privateKey: |
        <Paste the ssh private key of your authorized server here>
    zfs:
      datasetParentName: <Use the dataset name you created on the storage server, ie: hdd/nfs>
      detachedSnapshotsDatasetParentName: <Use the dataset name you created on the storage serverie: hdd/snapshots>
      datasetEnableQuotas: true
      datasetEnableReservation: false
      datasetPermissionsMode: "0777"
      datasetPermissionsUser: 0
      datasetPermissionsGroup: 0
    nfs:
      shareHost: <Put your TrueNAS FQDN or fixed IP here>
      shareAlldirs: false
      shareAllowedHosts: []
      shareAllowedNetworks: []
      shareMaprootUser: root
      shareMaprootGroup: wheel
      shareMapallUser: ""
      shareMapallGroup: ""
```

__ä½¿ç”¨helmå®‰è£åŠè¨­å®š__

```shell
helm upgrade \
  --install \
  --create-namespace \
  --values truenas-csi.yaml \
  --namespace democratic-csi \
  zfs-nfs democratic-csi/democratic-csi
```

__æª¢æŸ¥å®‰è£å¾Œçš„ç‹€æ…‹__

```shell
$ helm ls -Aa
```

__æª¢æŸ¥Podæ˜¯å¦running__

```shell
$ kubectl -n democratic-csi get pods
```

__æª¢æŸ¥storage classç‹€æ…‹__

```shell
$ kubectl get sc
```

__å¦‚æœæœ‰å¤šå€‹storage class, æœ‰åå¥½çš„storage classå¯è¨­ç‚ºé è¨­__

```shell
$ kubectl patch storageclass truenas-nfs-csi -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

## å®‰è£å¾Œæª¢æ ¸

__ç·¨è¼¯ä¸€å€‹pvc-20m.yaml__

```shell
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-20m
  namespace: default
spec:
  storageClassName: truenas-nfs-csi
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Mi
```

__Apply pvc-20m.yaml__

```shell
$ kubectl apply -f pvc-20m.yaml
```

__ç¢ºèªpvæ˜¯å¦å»ºç«‹__

```shell
$ kubectl get pv | grep pvc-20m
```

__ç·¨è¼¯zfs-pod.yaml,æœƒå»ºç«‹ä¸€å€‹podä¸¦å°‡volumeæ›è¼‰ä¸Š__

```shell
apiVersion: v1
kind: Pod
metadata:
  name: zfs-pod
  namespace: default
spec:
  containers:
  - image: ubuntu:22.04
    name: foo
    command:
    - tail
    - -f
    - /dev/null
    volumeMounts:
    - mountPath: /test-pvc
      name: test-pvc
  volumes:
  - name: test-pvc
    persistentVolumeClaim:
      claimName: pvc-20m
```

__Apply zfs-pod.yaml__

```shell
$ kubectl apply -f zfs-pod.yaml
```

__é€²å…¥podæª¢æŸ¥æ˜¯å¦æ›è¼‰volumeäº†__

```shell
$ kubectl exec -it zfs-pod -- bash

/* é€²å…¥å¾Œ */
$ df -h | grep test-pvc
```

__å»ºç«‹ä¸€å€‹æª”æ¡ˆå¾Œä¸¦åˆ°Truenasæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨äº†__

```shell
$ cd /test-pvc
$ touch hello
$ ls hello
hello
```

__æ¸¬å®Œå¾Œå°±å¯ä»¥åˆªé™¤å‰›æ¸¬è©¦è³‡æ–™__

```shell
kubectl delete pod zfs-pod
kubectl delete pvc pvc-20m
kubectl delete pv pvc-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

/* ç§»é™¤pvå¾Œé‚„è¦åˆ°Truenasåˆªé™¤è©²PVç›®éŒ„æ‰æœƒçœŸçš„åˆªé™¤ */
```

__é»é¸[Shares]->[åˆªé™¤]__

![Share-delete-0](./img/share-delete-0.png)

__é»é¸[Confirm]->[Continue]__

![Share-delete-1](./img/share-delete-1.png)

__é»é¸[Datasets], é¸å–è©²dataset,å†é»é¸[Delete]__

![Share-delete-2](./img/share-delete-2.png)

__è¼¸å…¥è©²datasetåç¨±, å†å‹¾é¸[Confirm]->[DELETE DATASET]__

![Share-delete-3](./img/share-delete-3.png)


- - -
# ğŸ”¥K8S å¸¸ç”¨æŒ‡ä»¤ç°¡ä»‹
- - -

## kubectlè¨­å®šèˆ‡è‡ªå‹•è£œå…¨

__kubectlè¨­å®š__

```shell
/* åœ¨user homeç›®éŒ„ä¸‹å»ºç«‹.kubeç›®éŒ„ */
$ mkdir .kube
$ sudo cp /etc/rancher/rke2/rke2.yaml .kube/config
$ sudo chown `whoami`:`whoami` .kube/config
$ echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' >> .profile
$ . ./.profile
```

__kubectlæŒ‡ä»¤è‡ªå‹•è£œå…¨__

```shell
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
$ echo 'alias k=kubectl' >>~/.bashrc
$ echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
$ . ./.bashrc
```

## kubectl Cheatsheet

[å®˜æ–¹å®Œæ•´æŒ‡ä»¤](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)

__list nodes and pods__

```shell
$ kubectl get nodes
$ kubectl get pod -A
$ kubectl get pod -A -o wide      # ç²å¾—è¼ƒå¤šè¼¸å‡ºè¨Šæ¯
```

__ç‚ºç¯€é»ä¸»æ©Ÿæ‰“ä¸Šlabel__

```shell
$ kubectl label node gpu-node node-role.kubernetes.io/1080Tigpu=
```

__è¨­å®šé‚£äº›ç¯€é»ä¸æœƒè¢«ç´å…¥podçš„æ’ç¨‹__

```shell
$ kubectl taint nodes master1 node-role.kubernetes.io/controlplane=true:NoSchedule
$ kubectl describe node master1 | grep Taints      # æŸ¥çœ‹taintæ˜¯å¦å­˜åœ¨
$ kubectl taint nodes master1 node-role.kubernetes.io/controlplane=true:NoSchedule-   # å–æ¶ˆtaint
```

__é€²å…¥podæ“ä½œå‘½ä»¤__

```shell
$ kubectl exec -it -n kube-system etcd-master1 -- etcdctl version
```

## Troubleshooting

```shell
/* ä½¿ç”¨describeä¾†ç²å¾—Nodeæˆ–podçš„verbose output */

$ kubectl describe node
$ kubectl describe pod etcd-master1 -n kube-system

/* ä½¿ç”¨logsä¾†æŸ¥çœ‹podä¸­çš„logè¼¸å‡º */

$ kubectl logs etcd-master1 -n kube-system

/* ä½¿ç”¨topä¾†æŸ¥çœ‹è³‡æºè€—ç”¨ */

$ kubectl top node
$ kubectl top pod -A


```
### RKE2 logsä½ç½®

__System__

  RKE2 ä»¥systemdæœå‹™é‹è¡Œ,æ•…å¯ä»¥æŸ¥çœ‹syslogçš„logè¨˜éŒ„

```shel
/var/log/syslog
```

__Kubelet__

å¯ä»¥ç”¨PSæŸ¥çœ‹kubeletå•Ÿå‹•æ™‚æ‰€å¸¶åƒæ•¸

```shell
/var/lib/rancher/rke2/agent/logs/kubelet.log
```

__Server__

```shell
journalctl -u rke2-server -f
systemctl status rke2-server.service
```

__MISC__

```shell
 /var/log/containers
 /var/log/pods